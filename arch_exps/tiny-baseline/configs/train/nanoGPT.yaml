logging:
  wandb_log: True
  wandb_project: "TinyUniverse"
  wandb_run_name: "nanoGPT"


arch:
  context_window: 512
  vocab_size: 50304
  depth: 12
  hidden_dim: 768
  num_heads: 12
  mlp_dim: 3072
  dropout: 0.0
  bias: False


training:
  dataset: "en_wiki"
  tokenizer: "gpt2_tokenizer"
  seed: 489
  batch_size: 12
  gradient_accumulation_steps: 40
  max_iters: 600000
  lr_decay_iters: 600000
  warmup_steps: 1000
  eval_interval: 2000


  optimizer:
    type: "AdamW"
    lr: 6e-4
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: True 
    warmup_iters: 2000
    min_lr: 6e-5


  


