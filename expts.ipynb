{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "def label_func(f): return f[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "# learn.fine_tune(1, 3e-3)\n",
    "# res = learn.validate()\n",
    "# res \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.unlabeled import OpenWebTextConfig, TiktokenTokenizer\n",
    "from model.gpt2 import GPT\n",
    "from train import OWTData, RandomSubsetSampler, save_model_checkpoints\n",
    "import numpy as np\n",
    "from fastai.text.all import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.customLearner import customLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TiktokenTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs = 16\n",
    "block_size = 512\n",
    "valid_sampler_size = 1000 #how many samples to use for validation. This is only used to check if validation loss is better than best_valid_loss, so that a checkpoint can be saved. Karpathy uses 200 random points\n",
    "\n",
    "\n",
    "\n",
    "model = GPT(block_size=block_size)\n",
    "\n",
    "train_ds = OWTData(OpenWebTextConfig().default_cache_dir/'train.bin'    ,block_size=block_size, dtype = tokenizer._get_numpy_dtype())\n",
    "valid_ds = OWTData(OpenWebTextConfig().default_cache_dir/'val.bin'      ,block_size=block_size, dtype = tokenizer._get_numpy_dtype())\n",
    "                    \n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_size=2*bs,\n",
    "                        sampler = RandomSubsetSampler(valid_ds, subset_size=valid_sampler_size),\n",
    "                        )\n",
    "\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "dls.c = model.head.out_features\n",
    "\n",
    "\n",
    "check_and_save_model = save_model_checkpoints(dir = 'checkpoints', \n",
    "                                                model_name = str(model), \n",
    "                                                checkpoint_name = 'gpt2', \n",
    "                                                every_iters = 10000)\n",
    "\n",
    "learn = customLearner(dls, \n",
    "                model, \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=[accuracy, Perplexity()],\n",
    "                cbs = [check_and_save_model],\n",
    "                ).to_bf16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dl))\n",
    "tokenizer.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
