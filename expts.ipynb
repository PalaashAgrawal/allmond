{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data.config.WikipediaSimpleConfig'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 10 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|██████████| 205328/205328 [00:01<00:00, 106988.87 examples/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from data.config import config_dict\n",
    "config = config_dict['wikisimple']\n",
    "print(config)\n",
    "data = datasets.load_dataset(\n",
    "                    # path = str(config.default_cache_dir), \n",
    "                    config.dataset_name,\n",
    "                    data_dir = config.default_cache_dir,\n",
    "                    num_proc=10,\n",
    "                    trust_remote_code=True,\n",
    "                    cache_dir=config.default_cache_dir,\n",
    "                    download_mode ='reuse_cache_if_exists',\n",
    "                    **getattr(config, 'kwargs',{}),\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrawalp2/miniconda3/envs/tinylm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "from model.gpt import GPT\n",
    "from data.unlabeled import download_dataset\n",
    "from data.loader import memmapDL, distributedMemmapDL\n",
    "\n",
    "from model.gpt import GPT\n",
    "\n",
    "from learner.callbacks import save_checkpoints\n",
    "from learner.LLMLearner import LLMLearner\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.distributed import *\n",
    "from fastai.callback.wandb import *\n",
    "import wandb\n",
    "\n",
    "#________________________________________wandb____________________________________________\n",
    "log_wandb = False #set to False if you dont want to log progress to W&B\n",
    "\n",
    "project = 'tinylm' #for wandb\n",
    "dataset = \"wikisimple\"\n",
    "mode = 'scratch'\n",
    "# ________________________________________hyperparams and settings_________________________\n",
    "\n",
    "bs=1 #each GPU gets bs = 20, works good for a 24GB GPU\n",
    "model_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "valid_sampler_size = 1000 #how many samples to use for validation. This is only used to check if validation loss is better than best_valid_loss, so that a checkpoint can be saved. Karpathy uses 200 random points\n",
    "validate_every = 1000 #1000 iterations, each iteration is bs*total_GPUs inputs\n",
    "block_size = 1024\n",
    "qlora = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT.from_hf('microsoft/Phi-3-mini-4k-instruct', enable_qlora=True) \n",
    "model = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.66k/1.66k [00:00<00:00, 3.66MB/s]\n",
      "Downloading: 100%|██████████| 235M/235M [00:15<00:00, 15.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path, valid_path = rank0_first(lambda: download_dataset(dataset = dataset, encoder = model.tokenizer)) #check if data exists, download only for rank0 GPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = memmapDL(train_path, bs = bs, block_size=model.block_size, \n",
    "                      dtype=model.tokenizer._get_numpy_dtype())\n",
    "valid_dl = memmapDL(valid_path, bs = bs, block_size=model.block_size, \n",
    "                      dtype=model.tokenizer._get_numpy_dtype(), \n",
    "                      sample_size = valid_sampler_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT()\n",
    "l=LLMLearner(dls,model,opt_func = partial(OptimWrapper, opt=torch.optim.AdamW),\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=[accuracy, Perplexity()],)\n",
    "l.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying 64\n",
      "trying 32\n",
      "trying 16\n",
      "trying 8\n",
      "Detected largest batch_size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='4788' class='' max='6670794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.07% [4788/6670794 47:13&lt;1095:51:28 7.2995]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "l.fit(1, find_largest_batch_size = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
