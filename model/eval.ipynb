{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrawalp2/miniconda3/envs/tinylm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<00:00, 10.1MB/s]\n",
      "/home/agrawalp2/miniconda3/envs/tinylm/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-06-05:01:57:18,439 WARNING  [cextension.py:101] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from .transformer_components import TransformerBlock\n",
    "from .huggingface_wrappers import HuggingfaceModelWrappers\n",
    "from .tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from lm_eval.api.model import LM #for \n",
    "\n",
    "    \n",
    "class model_utils:\n",
    "    \n",
    "    @property\n",
    "    def device(self): return next(self.parameters()).device\n",
    "\n",
    "    def __str__(self): \n",
    "        f'get model name along with number of parameters in millions/billions'\n",
    "        def _format_number(num):\n",
    "            if num >= 1_000_000_000:\n",
    "                return f\"{num / 1_000_000_000:.1f}B\"\n",
    "            elif num >= 1_000_000:\n",
    "                return f\"{num / 1_000_000:.1f}M\"\n",
    "            else:\n",
    "                return str(num)\n",
    "        \n",
    "        model_name = self.model_name if hasattr(self,'model_name') else 'GPT'\n",
    "            \n",
    "        return f'{model_name}-{_format_number(self.num_params)}'\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = self.num_params\n",
    "        assert hasattr(self, 'wpe') and isinstance(self.wpe, nn.Embedding), f'Positional Encoding Embedding (wpe) not defined in the model. Define `self.wpe` as a nn.Embedding'\n",
    "        assert hasattr(self, 'wte') and isinstance(self.wte, nn.Embedding), f'Token Embedding layer (wte) not defined in the model. Define `self.wte` as a nn.Embedding'\n",
    "        \n",
    "        if non_embedding: n_params-=self.wpe.weight.numel()\n",
    "        params_excl_embeddings = n_params - self.wte.weight.numel()\n",
    "        \n",
    "        print(f'Number of parameters: {n_params/1e6:.2f}M. Number of parameters (excluding embeddings): {params_excl_embeddings/1e6:.2f}M. Embeddings occupy {params_excl_embeddings/n_params*100:.2f}% of the total parameter count. ')\n",
    "        \n",
    "        return n_params\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _residual_init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for residual connections. Reweight std deviation according to GPT2 paper.\n",
    "        The remaining layers are default initialized according to Pytorch. I don't think 0.02 stddev is a necessary condition (Acc to original GPT paper (2018), they say 0.02 \"works well\", without any proper justification)\n",
    "        \"\"\"\n",
    "        for param_name, param in self.named_parameters():\n",
    "            if param_name.endswith(('residual_fc.weight', 'residual_projection.weight')): param.div_(torch.sqrt(torch.tensor(2*self.n_layer)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class GPT(nn.Module, HuggingfaceModelWrappers, model_utils):\n",
    "    \n",
    "    model_name = 'GPT'\n",
    "    def __init__(self,\n",
    "                block_size: int = 1024,\n",
    "                vocab_size: int = 50304, # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency,\n",
    "                n_layer: int = 12,\n",
    "                n_head: int = 12,\n",
    "                n_embd: int = 768,\n",
    "                dropout: float = 0.0,\n",
    "                bias: bool = True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster,\n",
    "                \n",
    "                tokenizer_from: str = 'gpt2',\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the GPT-2 model.\n",
    "\n",
    "        Args:\n",
    "            block_size (int): The size of each block.\n",
    "            vocab_size (int): The vocabulary size.\n",
    "            n_layer (int): The number of transformer layers.\n",
    "            n_head (int): The number of attention heads.\n",
    "            n_embd (int): The dimension of the token embeddings and the positional embeddings.\n",
    "            dropout (float): The dropout rate.\n",
    "            bias (bool): Whether to include bias in Linears and LayerNorms.\n",
    "            \n",
    "            tokenizer_from (str): By default, we use the Tiktoken Tokenizer. This parameter is used to specify  which model to source the tokenizer from (as supported by TikToken). Default to the gpt2 tokenizer, which contains 50,304 tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        self.wte = nn.Embedding(vocab_size, n_embd) # token embedding\n",
    "        self.wpe = nn.Embedding(block_size, n_embd) # positional embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(n_embd, n_head, dropout, bias, \n",
    "                                                      apply_causal_mask = True, \n",
    "                                                      block_size=self.block_size) \n",
    "                                     for _ in range(n_layer)])\n",
    "        \n",
    "        self.layernorm_final = nn.LayerNorm(n_embd, bias = bias)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias = bias)\n",
    "        #weight tying\n",
    "        self.wte.weight = self.head.weight\n",
    "        self._residual_init_weights() #per GPT2 paper\n",
    "        self.tokenizer = Tokenizer(tokenizer_from)\n",
    "        self.forward_fn = self._gpt_forward_impl #we separately define forward_fn so that custom defined huggingface models can easily implement their forwarding.\n",
    "        \n",
    "        \n",
    "    def _gpt_forward_impl(self, idx):\n",
    "        f'implentation of the forward function for the generic GPT class.'\n",
    "        f'Educational Note: as you see, this function in invariant to the sequence length. The only reason padding is done, is so that input sequences can be processed in batches.'\n",
    "        \n",
    "        _,t = idx.shape #idx = b,t\n",
    "        assert t<=self.block_size, f'Cannot forward -- model block size is exhausted. Model block size is {self.block_size}, but input sequence length is {t}.'\n",
    "        pos = torch.arange(t, dtype = torch.long, device = idx.device) #shape (t,)\n",
    "        \n",
    "        x = self.wpe(pos) + self.wte(idx) #t,n_embd + b,t,n_embd --> b,t,n_embd\n",
    "        x = self.dropout(x) #b,t,n_embd\n",
    "        for block in self.layers: x = block(x) #b,t,n_embd\n",
    "        x = self.layernorm_final(x) #b,t,n_embd\n",
    "        return self.head(x) #b,t,vocab_size\n",
    "        \n",
    "        \n",
    "    @torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False)\n",
    "    def forward(self, idx): \n",
    "        assert hasattr(self, 'forward_fn'), f'You need to implement a forward_fn function as attribute for the model to process inputs.'\n",
    "        assert isinstance(idx, torch.Tensor), f'forward function should only have one argument as input, i.e., the input tensor of shape (bs, seq_len)'\n",
    "        ret =  self.forward_fn(idx)\n",
    "        assert isinstance(ret, torch.Tensor), f'forward function should return a tensor. Instead got {type(ret)}'\n",
    "        \n",
    "        return ret\n",
    "        \n",
    "    \n",
    "          \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature = 1.0, top_k = None):\n",
    "        \"\"\"\n",
    "        Generate new tokens from the model., given a conditioning sequence of indices idx (LongTensor of shape (b,t)), and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time. \n",
    "        Most likely you'll want to make sure to be in model.eval() mode when calling this function.))\n",
    "        \n",
    "        TODO: padding mask for the input sequence., so that batch processing is possible\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #if the sequence context is growing too long we must crop it at block size\n",
    "            idx_cond = idx if idx.shape[1] <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            #take the logits of the last token and apply temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            #optinally choose only the top_k tokens\n",
    "            if top_k is not None:\n",
    "                v,_ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits<v[:, [-1]]] = -float('Inf')\n",
    "            #apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            #append sampled index to the running sequence and continuexw\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "        \n",
    "        return idx \n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def as_variant(cls, model_type:str, override_args:dict = None):\n",
    "        f'to TEST'\n",
    "        f\"\"\"\n",
    "        used to create an instance of the GPT model with a specific configuration based on the model_type parameter. \n",
    "        The model_type should be one of the following: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'. \n",
    "        These correspond to different configurations of the GPT model with varying numbers of layers, embedding dimensions, heads, vocabulary size, block size, and whether to use bias or not.\n",
    "        \"\"\"\n",
    "        supported_models = ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "        aliases = {'medium': 'gpt2-medium',\n",
    "                   'large': 'gpt2-large',\n",
    "                   'xl':    'gpt2-xl' \n",
    "                   }\n",
    "        assert model_type in supported_models, f'Unsupported model type. Supported variant model types are: {supported_models}'\n",
    "        if override_args is None: override_args = {} \n",
    "        assert all(k=='dropout' for k in override_args) #only dropout is overridable for now. According to Karpathy's repo. \n",
    "        \n",
    "        config_args  = {'gpt2':         {'n_layer': 12, 'n_embd': 768,  'n_head': 12,   'vocab_size': 50257,  'block_size': 1024, 'bias': True}, #124M params\n",
    "                        'gpt2-medium':  {'n_layer': 24, 'n_embd': 1024, 'n_head': 16,   'vocab_size': 50257,  'block_size': 1024, 'bias': True}, #345M params\n",
    "                        'gpt2-large':   {'n_layer': 36, 'n_embd': 1280, 'n_head': 20,   'vocab_size': 50257,  'block_size': 1024, 'bias': True}, #774M params\n",
    "                        'gpt2-xl':      {'n_layer': 48, 'n_embd': 1600, 'n_head': 25,   'vocab_size': 50257,  'block_size': 1024, 'bias': True}, #1558M params\n",
    "                        }[model_type]\n",
    "        \n",
    "        _model =  cls(**config_args, **override_args)\n",
    "        _model.model_name = model_type\n",
    "        return _model\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_hf(cls, model_identifier, enable_qlora:bool = False, **kwargs):\n",
    "        f\"\"\"Create an instance of the GPT model from a Huggingface model identifier. \n",
    "        The model_identifier should be a string that corresponds to a model in the Huggingface model hub.\n",
    "        Basically this model will behave exactly like the GPT class, except that the model parameters will be loaded from the Huggingface model hub.\n",
    "        \n",
    "        kwargs in this case are set as attributes of the GPT model instance.\n",
    "        \"\"\"\n",
    "                \n",
    "        instance = cls.__new__(cls)\n",
    "        super(cls, instance).__init__() #for nn.Module\n",
    "        instance.qlora = enable_qlora==True\n",
    "        \n",
    "        hf_model  = instance.get_hf_model(model_identifier, enable_qlora = enable_qlora)\n",
    "        for key, value in hf_model.cfg_dict.items(): setattr(instance, key, value)\n",
    "        #storing the model parameters in the class instance\n",
    "        instance.base_model = hf_model  \n",
    "        #defining the forward_fn for proper forwarding. \n",
    "        instance.forward_fn = lambda x: instance.base_model(x).logits\n",
    "        \n",
    "        for key, value in kwargs.items(): setattr(instance, key, value)\n",
    "        return instance\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
