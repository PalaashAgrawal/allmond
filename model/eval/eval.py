#utils for evaluation. These functions are required by lm-evaluation-harness

from lm_eval.api.model import LM 
from lm_eval.api.instance import Instance
from lm_eval.evaluator import simple_evaluate
from lm_eval.utils import make_table

from pathlib import Path
from copy import deepcopy
from typing import List
import torch
from tqdm import tqdm
import json
from fastai.torch_core import rank_distrib, num_distrib


class evalUtils:
    def _encode_pair(self, context, continuation):
        n_spaces = len(context) - len(context.rstrip())
        if n_spaces > 0:
            continuation = context[-n_spaces:] + continuation
            context = context[:-n_spaces]
        whole_enc = self.tokenizer.encode(context + continuation)
        context_enc = self.tokenizer.encode(context)
        context_enc_len = len(context_enc)
        continuation_enc = whole_enc[context_enc_len:]
        return context_enc, continuation_enc

    
    def _loglikelihood_tokens(self, requests, disable_tqdm=False):        
        res = []
        pbar = tqdm(
            total=len(requests),
            disable=(disable_tqdm or (self.rank != 0)),
            desc="Running loglikelihood requests",
        )
        
        for (context, continuation), context_enc, continuation_enc in requests:
            inp = (context_enc + continuation_enc)[-(self.max_length - 1) :]
            output = self.generate(inp, max_new_tokens = 0, return_logprobs=True)
            logprob = sum(output).tolist()
            is_greedy = self.is_greedy
            res.append((logprob, is_greedy))
            pbar.update(1)
        
        pbar.close()
        return res
    
    
    def get_rolling_token_windows(self, token_list, prefix_token, max_seq_len, context_len):
        """
        - context_len allows for a rolling window context, allowing each prediction window to potentially
        condition on some context

        :param token_list: list
            List of tokens to be PREDICTED
        :param max_seq_len: int
            max_seq_len of model (or max_seq_len we want to use)
        :param context_len: int
            Amount of desired token context for prediction. Needs to be at least 1.
        :param prefix_token: token
            Dummy token like <eos> so the first token has something to condition on
        :return: generator
            Generator of tuples
                (input_tokens, pred_tokens)
            Note: Score only the last len(pred_tokens) logits of the LM
        """
        assert 1 <= context_len <= max_seq_len
        if not token_list: return
        # +1 offset, going from input->preds
        pred_len = max_seq_len - context_len + 1
        predicted = 0

        # Special handling for first window: predict all tokens
        first_seq_len = min(max_seq_len, len(token_list))
        yield ([prefix_token] + token_list[: first_seq_len - 1], token_list[:first_seq_len])
        predicted += first_seq_len

        while predicted < len(token_list):
            window_pred_len = min(len(token_list) - predicted, pred_len)
            window_end = predicted + window_pred_len

            yield (
                token_list[window_end - max_seq_len - 1 : window_end - 1],
                token_list[window_end - window_pred_len : window_end],
            )
            predicted += window_pred_len

    def make_disjoint_window(self, pair):
        """Takes output from get_rolling_token_windows and makes the context not overlap with the continuation"""
        a, b = pair
        return a[: len(a) - (len(b) - 1)], b

    
    
    @property
    def eot_token_id(self): return self.tokenizer.eot_token
    
    @property
    def max_length(self): return self.block_size
    
    #give ability to set max_length property
    @max_length.setter
    def max_length(self, value): self.block_size = value
    
    def tok_encode(self, text): return self.tokenizer.encode(text)

    


class evalBase(LM, evalUtils):
    
    @torch.no_grad()
    def loglikelihood(self, requests:list[Instance])-> list[tuple[float, bool]]:
        """
        This function is required by simple_evaluate to carry out evaluation. 
        
        Each request contains Instance.args : Tuple[str, str] containing 1. an input string to the LM and 2. a target string on which the loglikelihood of the LM producing this target, conditioned on the input, will be returned.
        Each request will have, as result, (ll, is_greedy): Tuple[float, int] returned, where ll is a floating point number representing the log probability of generating the target string conditioned on the input, and is_greedy being either the value 0 or 1, with it being 1 if and only if the target string would be generated by greedy sampling from the LM (that is, if the target string is the most likely N-token string to be output by the LM given the input. )
        """
        
        new_reqs = []
        for context, continuation in [req.args for req in requests]:
            if context == "":
                # end of text as context
                context_enc, continuation_enc = (
                    [self.eot_token_id],
                    # self.tok_encode(continuation),
                    self.tokenizer.encode(continuation),
                )
            else:
                context_enc, continuation_enc = self._encode_pair(context, continuation) #more efficient to encode them together, rather than individually. You can always concat and split

            new_reqs.append(((context, continuation), context_enc, continuation_enc))

        return self._loglikelihood_tokens(new_reqs)
    
    
    def loglikelihood_rolling(
        self, requests: List[Instance], disable_tqdm: bool = False
    ) -> List[float]:
        """
        Compute full log-likelihood of a string, with no truncation, for perplexity computation
        - We will use the full max context length of the model.
        - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to
        the max context length.
        
        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context,).
            string: str
                String for which we are computing overall loglikelihood
        :return: list[tuple[float]]
            A list of tuples (logprob,)
            logprob: float
                The log probability of `context` conditioned on the BOS/EOS token.
                Can also be overridden for custom cases by `prefix_token_id`.
        """
      
        loglikelihoods = []

        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):
            # Tokenize the input string
            token_list = self.tokenizer.encode(string)
            
            # Get rolling token windows
            rolling_token_windows = list(self.get_rolling_token_windows(
                token_list=token_list,
                prefix_token=self.eot_token_id,
                max_seq_len=self.max_length,
                context_len=1,
            ))

            # Make disjoint windows
            rolling_token_windows = [self.make_disjoint_window(x) for x in rolling_token_windows]

            # Compute log-likelihood for each window
            string_nll = self._loglikelihood_tokens(
                rolling_token_windows,
            )

            # Discard is_greedy
            string_nll = [x[0] for x in string_nll]

            # Sum the log-likelihoods for the entire string
            string_nll = sum(string_nll)
            loglikelihoods.append(string_nll)

        return loglikelihoods

    
    
    def generate_until(self, requests):
        """Generate greedily until a stopping sequence

        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context, until).
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list[str]
            A list of strings continuation
            continuation: str
                The generated continuation.
        """
        if not requests: return []
    
        res = []

        def get_until(req_args):
            until = req_args.get("until", [])
            until = deepcopy(until)  # prevent from modifying req_args for cache_key
            if self.tokenizer.ids_to_tokens([self.eot_token_id])[0] not in until:
                until.append(self.tokenizer.ids_to_tokens([self.eot_token_id])[0])
            return until

        for request in requests:
            context, req_args = request.args
            until = get_until(req_args)
            max_gen_toks = req_args.get("max_gen_toks", self.max_gen_toks)
            
            remaining_length = self.max_length - max_gen_toks
            encoded_context = self.tokenizer.encode(context)
            encoded_context = encoded_context[-remaining_length:]
            context_str = self.tokenizer.decode(encoded_context)

            generated_text = context_str
            for _ in range(max_gen_toks):
                output = self.generate(
                    generated_text,
                    max_new_tokens=1,
                    temperature=0,  # greedy generation
                    top_k=None,
                    return_input=False
                )

                generated_text += self.tokenizer.decode(output)

                stop = False
                for term in until:
                    if term in generated_text:
                        generated_text = generated_text.split(term)[0]
                        stop = True
                        break

                if stop: break

            self.cache_hook.add_partial("greedy_until", request, generated_text)
            res.append(generated_text)

        return res

    def evaluate(self, tasks:list[str], save_path = None):
        """
        run model evaluation on bencharks (as defined by Eleuther AI's lm-evaluation-harness)
        define your tasks as list of strings
        
        save results as dict (json) at `save_path` for later visualization. Defaults to None
        if save_path is None: results are not saved, but still displayed as a table
        """
        #insert assertions to verify if strings in benchmarks is valid
        #TODO: make this cleaner. save_path should automatically be sourced from Learner's save paths/model_dir
        
        print('running eval. This may take a few minutes just to setup...')    
        results = simple_evaluate(self, tasks = tasks, batch_size = 'auto')
        if save_path is not None:
            pth = Path(save_path)/f'{str(self)}_eval.json'
            print('saving results at', pth)
            with open(pth, 'w') as f: json.dump(results, f)
        
        print(make_table(results))
        
    
    
    
    def _detect_batch_size(self):
        max_length = self.max_length

        # Starting with a relatively high batch size and reducing it in case of OOM errors
        batch_size = 64  # Initial batch size
        step_size = 2  # Reduction factor in case of OOM

        def can_allocate_memory(batch_size):
            try:
                # Create a dummy input to test memory allocation
                test_batch = torch.ones((batch_size, max_length), device=self.device).long()
                # Run a forward pass
                self.generate(test_batch, max_new_tokens=0)
                return True
            except RuntimeError as e:
                if "out of memory" in str(e):
                    return False
                else:
                    raise e

        # Adjust the batch size until it fits into memory
        while batch_size > 0:
            if can_allocate_memory(batch_size):
                break
            batch_size //= step_size

        # Ensure at least one batch can be processed
        return max(batch_size, 1)
    
    
    
        
    @property
    def rank(self): return rank_distrib()
    
    @property
    def world_size(self): return num_distrib() or 1
    
    
    