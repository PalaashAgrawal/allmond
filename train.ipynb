{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrawalp2/miniconda3/envs/tinylm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.unlabeled import OpenWebTextConfig, TiktokenTokenizer\n",
    "from model.gpt2 import GPT\n",
    "from train import OWTData, RandomSubsetSampler, save_model_checkpoints\n",
    "import numpy as np\n",
    "\n",
    "from fastai.text.all import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TiktokenTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs = 16\n",
    "block_size = 512\n",
    "valid_sampler_size = 1000 #how many samples to use for validation. This is only used to check if validation loss is better than best_valid_loss, so that a checkpoint can be saved. Karpathy uses 200 random points\n",
    "\n",
    "\n",
    "\n",
    "model = GPT(block_size=block_size)\n",
    "\n",
    "train_ds = OWTData(OpenWebTextConfig().default_cache_dir/'train.bin'    ,block_size=block_size, dtype = tokenizer._get_numpy_dtype())\n",
    "valid_ds = OWTData(OpenWebTextConfig().default_cache_dir/'val.bin'      ,block_size=block_size, dtype = tokenizer._get_numpy_dtype())\n",
    "                    \n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_size=2*bs,\n",
    "                        sampler = RandomSubsetSampler(valid_ds, subset_size=valid_sampler_size),\n",
    "                        )\n",
    "\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "dls.c = model.head.out_features\n",
    "\n",
    "check_and_save_model = save_model_checkpoints(dir = 'models', \n",
    "                                                model_name = str(model), \n",
    "                                                checkpoint_name = 'gpt2', \n",
    "                                                every_iters = 10000)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model, \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=[accuracy, Perplexity()],\n",
    "                cbs = [check_and_save_model],\n",
    "                ).to_bf16()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
